apiVersion: apps/v1
kind: Deployment
metadata:
  name: conbench-deployment
spec:
  selector:
    matchLabels:
      app: conbench
  replicas: 2
  # the number of seconds you want to wait for your Deployment to progress
  # before the system reports back that the Deployment has failed progressing -
  # surfaced as a condition with type: Progressing, status: "False". and
  # reason: ProgressDeadlineExceeded in the status of the resource.
  # Defaults to 600 (10 min), give this 15 mins here.
  progressDeadlineSeconds: 900
  # `minReadySeconds`: Defaults to 0: The Pod will be considered available as
  # soon as it is ready. However, wait a bit for things to equilibrate, this is
  # also important for Service-based request routing (we don't want to receive
  # requests too early).
  minReadySeconds: 10
  strategy:
    type: RollingUpdate
    rollingUpdate:
      # The number of pods that can be created above the desired amount of pods
      # during an update. That is, with `replicas: 2` and `maxSurge: 1` we
      # can have at most three pods at the same time.
      maxSurge: 1
      # Make it so that during the update at least one pod is ready.
      maxUnavailable: 1
  template:
    metadata:
      labels:
        app: conbench
    spec:
      containers:
      - name: conbench
        image: "{{DOCKER_REGISTRY}}/{{FLASK_APP}}:{{BUILDKITE_COMMIT}}"
        command: [
          "gunicorn", "-b", "0.0.0.0:5000",
          "-w", "5",
          "-t", "120",
          "conbench:application",
          "--access-logfile=-",
          # Add L specifier to access log, to log request processing duration. Also see
          # https://github.com/conbench/conbench/issues/810
          "--access-logformat", '%(h)s %(l)s %(u)s %(t)s "%(r)s" %(s)s %(b)s "%(f)s" "%(a)s" (%(L)s s)',
          "--error-logfile=-",
          "--preload"
          ]
        imagePullPolicy: "Always"
        ports:
          - name: gunicorn-port
            containerPort: 5000
        envFrom:
          - configMapRef:
              name: conbench-config
          - secretRef:
              name: conbench-secret
        resources:
          requests:
            memory: '500Mi'
            cpu: 0.2
        readinessProbe:
          failureThreshold: 1
          httpGet:
            path: /api/ping/
            port: 5000
            scheme: HTTP
          initialDelaySeconds: 60
          periodSeconds: 10
          successThreshold: 2
          timeoutSeconds: 20
      terminationGracePeriodSeconds: 60
---
apiVersion: v1
kind: Service
metadata:
  name: "conbench-service"
  labels:
    app: conbench
  annotations:
    # `kubectl describe ingress -n staging` shows `FailedDeployModel`
    #  With error detail "Failed deploy model due to InvalidParameter: 1
    # validation error(s) found: minimum field value of 1,
    # CreateTargetGroupInput.Port." Searched. Found
    # https://github.com/kubernetes-sigs/aws-load-balancer-controller/issues/1695#issuecomment-850411558
    # which suggests as solution the `target-type: ip` below.
    alb.ingress.kubernetes.io/target-type: ip
spec:
  ports:
  - name: conbench-service-port
    port: 80
    # The port number above: the port that will be exposed by this service.
    # `targetPort` is documented with "Number or name of the port to access on
    # the pods targeted by the service."
    targetPort: gunicorn-port
    protocol: TCP
  type: NodePort
  selector:
    app: "conbench"
---
apiVersion: monitoring.coreos.com/v1
# This is a CRD defined by the prometheus-operator/kube-prometheus project
kind: ServiceMonitor
metadata:
  name: conbench-service-monitor
spec:
  selector:
    matchLabels:
      app: conbench
  endpoints:
    # It's important that ServiceMonitor i) refers to a Service port  and ii)
    # refers to it via its name.
    - port: conbench-service-port
      path: /metrics
      scheme: http
      # This defines the scrape interval, and therefore the time resolution
      # within timeseries (usually the moving window width, over which
      # aggregates are reported). 30 s is already on the lower end, others
      # sometimes choose 1-5 mins, in the Prometheus ecosystem.
      interval: 30s
---
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  # Note(JP): we do not seem to use the k8s external DNS mechanism. Terraform
  # logic looks for the corresponding ingress object with the filter
  # `name=conbench-ingress`, and then updates DNS records (via Route 53) behind
  # the scenes to point to the load balancer (ALB) corresponding to this
  # ingress object. That is, the mapping between a specific DNS name and the
  # ALB happens in a different repository.
  name: "conbench-ingress"
  annotations:
    kubernetes.io/ingress.class: alb
    alb.ingress.kubernetes.io/scheme: internet-facing
    # Introduce the idea of IngressGroup (supported in
    # aws-load-balancer-controller 2.x), via which multiple k8s ingress objects
    # (also spread across multiple k8s namespaces) can configure the _same_ ALB
    # in AWS.
    alb.ingress.kubernetes.io/group.name: cb-alb-group
    # allow for higher-prio config elsewhere, this can be important for path
    # evaluation rules.
    alb.ingress.kubernetes.io/group.order: "9"
    # Configure health check against Conbench containers
    alb.ingress.kubernetes.io/healthcheck-protocol: HTTP
    alb.ingress.kubernetes.io/healthcheck-path: /api/ping/
    alb.ingress.kubernetes.io/healthcheck-interval-seconds: '30'
    alb.ingress.kubernetes.io/success-codes: '200'
    # Configure exposure to Internet
    alb.ingress.kubernetes.io/listen-ports: '[{"HTTP": 80}, {"HTTPS":443}]'
    alb.ingress.kubernetes.io/actions.ssl-redirect: '{"Type": "redirect", "RedirectConfig": { "Protocol": "HTTPS", "Port": "443", "StatusCode": "HTTP_301"}}'
    alb.ingress.kubernetes.io/certificate-arn: {{CERTIFICATE_ARN}}
  labels:
    app: conbench-ingress
spec:
  rules:
  - http:
      paths:
      - path: /*
        pathType: ImplementationSpecific
        backend:
          service:
            name: ssl-redirect
            port:
              name: use-annotation
      - path: /
        pathType: Prefix
        backend:
          service:
            name: conbench-service
            port:
              number: 80

